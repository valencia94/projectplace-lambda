# .github/workflows/deploy_and_convert.yml
# ------------------------------------------------------------
name: Deploy and Convert - stag              # parens removed â†’ avoid quoting issues
on:
  push:
    branches: [ main ]
  workflow_dispatch: {}

# â”€â”€ global constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
env:
  AWS_REGION:     us-east-2
  ECR_REGISTRY:   703671891952.dkr.ecr.us-east-2.amazonaws.com
  ECR_REPOSITORY: projectplace-lambda
  STAGE_TAG:      stag                       # literal; no ${{ â€¦ }}

concurrency:                                # avoid parallel pushes stepping on each other
  group: deploy-and-convert-${{ github.ref }}
  cancel-in-progress: false

jobs:
  ################################################################
  # 1) Build container & update ProjectPlaceDataExtractor-stag
  ################################################################
  deploy-lambda:                          # â¬…ï¸ *inside* jobs:
    runs-on: ubuntu-latest

    steps:                                # â¬…ï¸ correctly indented under job
    - name: ðŸ“¥ Checkout
      uses: actions/checkout@v3

    - name: ðŸ” Configure AWS creds
      uses: aws-actions/configure-aws-credentials@v2
      with:
        aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region:            ${{ env.AWS_REGION }}

    - name: ðŸ— Set up Docker Buildx        # new: safer multi-arch builds
      uses: docker/setup-buildx-action@v3

    - name: ðŸ”§ Export IMAGE_TAG
      run: |
        echo "IMAGE_TAG=${{ env.ECR_REGISTRY }}/${{ env.ECR_REPOSITORY }}:${{ env.STAGE_TAG }}" >>"$GITHUB_ENV"

    - name: ðŸ³ Login to ECR
      run: |
        aws ecr get-login-password --region "$AWS_REGION" |
        docker login --username AWS --password-stdin "$ECR_REGISTRY"

    - name: ðŸ›  Build & push (no-cache, linux/amd64)
      run: |
        set -euo pipefail
        docker buildx build \
          --no-cache --pull \
          --platform linux/amd64 \
          --build-arg USE_TAG_HANDLER=true \
          -t "$IMAGE_TAG" \
          --push .

    - name: ðŸš€ Update Lambda code
      run: |
        aws lambda update-function-code \
          --function-name ProjectPlaceDataExtractor-stag \
          --image-uri "$IMAGE_TAG" \
          --region "$AWS_REGION"

    - name: â³ Wait for code update to finish
      run: |
        aws lambda wait function-updated \
          --function-name ProjectPlaceDataExtractor-stag \
          --region "$AWS_REGION"

    # â”€â”€ Merge / upsert environment variables â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    - name: ðŸ“¦ Install jq
      run: sudo apt-get update -y && sudo apt-get install -y jq

    - name: ðŸ§¬ Merge & apply env vars
      run: |
        OLD=$(aws lambda get-function-configuration \
                --function-name ProjectPlaceDataExtractor-stag \
                --query 'Environment.Variables' --output json)

        NEW=$(echo "$OLD" | jq \
          --arg secret ProjectPlaceAPICredentials \
          --arg ddb   ProjectPlace_DataExtrator_landing_table_v3 \
          --arg s3    projectplace-dv-2025-x9a7b \
          '. + {
            SECRET_NAME:         $secret,
            DYNAMODB_TABLE_NAME: $ddb,
            S3_BUCKET_NAME:      $s3
          }')

        echo '{"Variables":'"$NEW"'}' > env.json

        aws lambda update-function-configuration \
          --function-name ProjectPlaceDataExtractor-stag \
          --environment file://env.json \
          --region "$AWS_REGION"

    - name: â³ Wait for env update
      run: |
        aws lambda wait function-updated \
          --function-name ProjectPlaceDataExtractor-stag \
          --region "$AWS_REGION"

    - name: ðŸ”Ž Kick Lambda (async) to verify
      run: |
        aws lambda invoke \
          --function-name ProjectPlaceDataExtractor-stag \
          --invocation-type Event \
          --payload '{}' \
          --region "$AWS_REGION" \
          /tmp/out.json
        echo "Lambda triggered â†’ $(cat /tmp/out.json)"

  ################################################################
  # 2) Convert generated DOCX files to PDF
  ################################################################
  convert-docx:                           # â¬…ï¸ same level as deploy-lambda
    runs-on: ubuntu-latest
    needs: deploy-lambda

    steps:
    - name: ðŸ“¥ Checkout
      uses: actions/checkout@v3

    - name: ðŸ” Configure AWS creds
      uses: aws-actions/configure-aws-credentials@v2
      with:
        aws-access-key-id:     ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region:            ${{ env.AWS_REGION }}

    - name: ðŸ— Install LibreOffice
      run: |
        sudo apt-get update -q
        sudo apt-get install -y libreoffice

    - name: ðŸ“¥ Wait & download Acta_*.docx
      run: |
        mkdir -p actas
        for i in {1..15}; do
          echo "â³ Attempt $i: checking for Acta_*.docx in S3..."
          aws s3 sync \
            s3://projectplace-dv-2025-x9a7b/actas actas \
            --exclude "*" --include "Acta_*.docx" --delete
          COUNT=$(ls -1 actas/Acta_*.docx 2>/dev/null | wc -l || true)
          if [ "$COUNT" -gt 0 ]; then
            echo "âœ… Found $COUNT .docx file(s)."
            break
          fi
          echo "ðŸ”„ Not ready. Sleeping 20 sâ€¦"
          sleep 20
        done

    - name: ðŸ–¨ Convert DOCX â‡’ PDF
      run: |
        cd actas
        for f in Acta_*.docx; do
          libreoffice --headless --convert-to pdf "$f"
        done

    - name: ðŸš€ Upload PDFs back to S3
      run: |
        aws s3 sync actas s3://projectplace-dv-2025-x9a7b/actas \
          --exclude "*" --include "Acta_*.pdf" \
          --content-type "application/pdf" \
          --content-disposition "attachment"
        echo "âœ… PDF upload complete."
